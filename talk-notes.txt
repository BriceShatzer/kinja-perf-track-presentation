Back in May a consultant with Google did some performance auditing on some of the Kinja sites and presented their findings.

-

They found that our performance was on par or better than some of our peers, but that where was still a lot things we could improve.

Their report pointed out a number of simple fixes and "easy wins" that we could implement pretty quickly, along with some bigger issues we should address. I'll talk about all that stuff in a little bit.


-
With the start of this quarter, Robin and myself now make up the performance team.

We're a part of the frontend future team. I assume everybody is aware, but they're big project is rewriting a bunch of our frontend stuff into react.
-

One of the benefits of that work is that it should make everything faster...so it makes sense for us to be working together...

-

So before we did anything else we needed to figure out what we actually wanted to try and accomplish.
-
The metrics decided that we initially want to care about are
*How long it takes for things to render on the page.
*The file size of the page.
*How many individual things are on the page.


-
The test cases (pages) we wanted to make sure we were paying attention to represented all the various types of pages & content that is available across our network.

Homepage of a non-satire site: https://gizmodo.com
Homepage of a satire site: https://www.theonion.com
Standard Post page with nearly every potenial blocknode: https://gawkerselenium.kinja.com/permanent-test-post-do-not-delete-1787626061
A Video Post: https://www.avclub.com/pacific-rim-uprising-proves-that-it-is-possible-to-scr-1823997019
live-blog post: https://gizmodo.com/well-be-liveblogging-the-apple-wwdc-2018-keynote-right-1826531789
Post using the Feaured Post templae: https://deadspin.com/native-american-lacrosse-teams-reported-racial-abuse-t-1824292659
Category Page: https://www.avclub.com/c/review

-

Next issue is consistency from test instance to instance
We're running all of these tests on live web pages and our
our ecosystem is complex enough that visiting the same page, could produce widely different results
so we wanted to control for that.

7 urls, 3 different variations



-

So we knew what we wanted to look, so now we had to figure out how we wanted to test that stuff.

Lighthouse! (show gizmodo audit tab)

but - automated (show dash)

(every 3 hours)
--

Established baseline of data,

we can also run individual one-off tests


regular - kinja deals

comparision - https://deadspin.com/?eu_disabled=on

--

I won't go into the details of how all this works,

basically we're using that lighthouse tool, but in AWS

working with the DevOps team to get up all this stuff up and running in our 

-- 


what are we doing?
easy:
Lazy-loaded images on homepages...didn't work like we thought that did
render blocking 3rd party JS files
font optimization
hard: 
Critical Path/Inline CSS & JS:  90% unused
Writing things to that page that don't get show (You May Also Like, on mobile)
some what related: "Privileged" functionality being served to everybody



